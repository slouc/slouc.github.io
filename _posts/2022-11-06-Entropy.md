---
layout: post
title: Language entropy
---

There are two extremes for a language in terms of its alphabet and its dictionary. 

One is that the entire alphabet consists of only one letter.
In that case, dictionary might looks like this: "a" (cat), "aa" (dog), "aaa" (cow), "aaaa" (goat), and so on. The length of the longest word is then equal to
the size of the dictionary.
The other extreme is a language that defines a different letter for every word (including all different tenses, cases etc).

Due to practical reasons, both of these two extremes are not really viable for humans to use in neither writing nor speech,
and we tend to go for a golden middle, with the alphabet size somewhere around 25-30.

Are all letters from the alphabet used equally frequently within the language? Of course not.
For example, "E" is the most frequent letter in both this sentence (14) and the whole text so far (99).
In both cases, "T" comes second, which isn't surprising given that some of the most common English words are 
"the", "to", "that", "it", "not", "with", "at", "this".

This brings us to entropy (another word with both "E" and "T"), also known as "average information content".
Smallest unit of information is one bit, which takes one of two possible values, 0 and 1.
So here's a question: which bit is more useful, one whose value is equally likely to be 0 and 1 (e.g. a coin toss),
or one where there is a much higher probability of one value over the other (e.g. "did you win the lottery")?
Former case (50-50) packs more information and therefore has the higher entropy 
(actually, the highest possible for a single bit).

Same goes for languages - if all letters are equally frequent, entropy is maximal.

This is Shannon's equation for entropy:

$$
E(X) = \sum_{x \in X} p(x) \log p(x)
$$

where `X` is the collection (e.g. a language), `x` is the single item (e.g. a letter), `p(x)`
is the probability of the occurrence of `x` within `X`, and `log` is the base two logarithm (we
could use any other base, but this gives us the unit of bits, or "shannons").

We can loosely translate this equation into the following algorithm for 
calculating the entropy of some piece of text:

```
1. Loop through all characters in the text:
   - Maintain the counter of occurrences for each letter, representing its frequency
2. Calculate the probability of each letter (frequency / total number of letters)
3. Loop through all letter probabilities:
   - Multiply each probability with its negative base-2 logarithm
4. Sum the values from previous step
```

This is the entropy of our input text. We could attempt approximate the
entropy for the whole language by using lots of huge text samples, but it would not be precise,
because a language is much more than just a collection of letters grouped
into words and sentences.
[Shannon's original paper](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)
was actually using [n-grams](https://en.wikipedia.org/wiki/N-gram), which doesn't come without
its own [set of limitations](https://clin2022.uvt.nl/limitations-of-the-entropy-measure-in-n-gram-language-modelling/).

Here's a question: How does entropy of a language change if
we destroy the fabric of grammar and language rules, and we simply arrange all the words in the
sorted order? Answer: this kind of normalisation results in approximately the same value
across all languages ([source](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0019875)). 

Here's another one: How quickly does entropy of a language converge to a stable value? Answer:
after an average of about 35k when using isolated-words aka "source" entropy, and 38k for
n-grams aka "block" entropy ([source](https://arxiv.org/pdf/1606.06996.pdf)).
Unsurprisingly, their correlation is linear:

![linear correlation](https://www.researchgate.net/profile/Christian-Bentz/publication/304277331/figure/fig2/AS:376016481603585@1466660723075/Correlation-between-block-and-source-entropies-for-the-PBC-texts.png)

What about speech? 

It has been shown that speech entropy runs at about ~39 bits/sec across
all languages ([source](https://www.science.org/doi/10.1126/sciadv.aaw2594)). This would indicate that,
regardless of the grammar, dictionary size or other parameters, we all converge to conveying
the same amount of information per second in spoken conversations.

